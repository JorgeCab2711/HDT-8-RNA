{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDT 8 \n",
    "- Mariana David\n",
    "- Alejandra Guzmán\n",
    "- Jorge Caballeros 20009\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use los mismos conjuntos de entrenamiento y prueba que utilizó en las hojas anteriores.\n",
    "\n",
    "\n",
    "2. Seleccione como variable respuesta la que creó con las categorías del precio de la casa."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LeakyReLU\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Genere dos modelos de redes neuronales que sean capaz de clasificar usando la variable \n",
    "respuesta que categoriza las casas en baratas, medias y caras. Estos modelos deben tener \n",
    "diferentes topologías y funciones de activación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('train.csv')\n",
    "data = pd.DataFrame(data)\n",
    "data.describe()\n",
    "# Preprocessing\n",
    "# Elimina columnas que no son relevantes o que contienen muchos valores faltantes\n",
    "data = data.drop(['Id', 'Alley', 'PoolQC', 'Fence', 'MiscFeature'], axis=1)\n",
    "\n",
    "# Elimina filas que tienen valores faltantes en columnas importantes\n",
    "data = data.dropna(subset=['SalePrice', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'LotFrontage', 'MasVnrType', 'MasVnrArea', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Electrical'])\n",
    "\n",
    "# Rellena los valores faltantes con la media o la moda de la columna\n",
    "data['LotFrontage'] = data['LotFrontage'].fillna(data['LotFrontage'].mean())\n",
    "data['MasVnrType'] = data['MasVnrType'].fillna(data['MasVnrType'].mode()[0])\n",
    "data['MasVnrArea'] = data['MasVnrArea'].fillna(data['MasVnrArea'].mean())\n",
    "data['BsmtQual'] = data['BsmtQual'].fillna(data['BsmtQual'].mode()[0])\n",
    "data['BsmtCond'] = data['BsmtCond'].fillna(data['BsmtCond'].mode()[0])\n",
    "data['BsmtExposure'] = data['BsmtExposure'].fillna(data['BsmtExposure'].mode()[0])\n",
    "data['BsmtFinType1'] = data['BsmtFinType1'].fillna(data['BsmtFinType1'].mode()[0])\n",
    "data['BsmtFinType2'] = data['BsmtFinType2'].fillna(data['BsmtFinType2'].mode()[0])\n",
    "data['Electrical'] = data['Electrical'].fillna(data['Electrical'].mode()[0])\n",
    "\n",
    "# Convierte las variables categóricas en variables numéricas\n",
    "data = pd.get_dummies(data)\n",
    "\n",
    "# Guarda los datos limpios en un nuevo archivo CSV\n",
    "data.to_csv('train_cleaned.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Use los modelos para predecir el valor de la variable respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
      "0          60         65.0     8450            7            5       2003   \n",
      "1          20         80.0     9600            6            8       1976   \n",
      "2          60         68.0    11250            7            5       2001   \n",
      "3          70         60.0     9550            7            5       1915   \n",
      "4          60         84.0    14260            8            5       2000   \n",
      "\n",
      "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  SaleType_New  \\\n",
      "0          2003       196.0         706           0  ...             0   \n",
      "1          1976         0.0         978           0  ...             0   \n",
      "2          2002       162.0         486           0  ...             0   \n",
      "3          1970         0.0         216           0  ...             0   \n",
      "4          2000       350.0         655           0  ...             0   \n",
      "\n",
      "   SaleType_Oth  SaleType_WD  SaleCondition_Abnorml  SaleCondition_AdjLand  \\\n",
      "0             0            1                      0                      0   \n",
      "1             0            1                      0                      0   \n",
      "2             0            1                      0                      0   \n",
      "3             0            1                      1                      0   \n",
      "4             0            1                      0                      0   \n",
      "\n",
      "   SaleCondition_Alloca  SaleCondition_Family  SaleCondition_Normal  \\\n",
      "0                     0                     0                     1   \n",
      "1                     0                     0                     1   \n",
      "2                     0                     0                     1   \n",
      "3                     0                     0                     0   \n",
      "4                     0                     0                     1   \n",
      "\n",
      "   SaleCondition_Partial  price_category  \n",
      "0                      0           caras  \n",
      "1                      0          medias  \n",
      "2                      0           caras  \n",
      "3                      0         baratas  \n",
      "4                      0           caras  \n",
      "\n",
      "[5 rows x 266 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0         caras\n",
       "1        medias\n",
       "2         caras\n",
       "3       baratas\n",
       "4         caras\n",
       "         ...   \n",
       "1089     medias\n",
       "1090      caras\n",
       "1091      caras\n",
       "1092     medias\n",
       "1093     medias\n",
       "Name: price_category, Length: 1094, dtype: category\n",
       "Categories (3, object): ['baratas' < 'medias' < 'caras']"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('train_cleaned.csv')\n",
    "# Definir los límites de cada categoría de precio\n",
    "lim_inf = data['SalePrice'].quantile(0.33)\n",
    "lim_sup = data['SalePrice'].quantile(0.66)\n",
    "\n",
    "# Crear una nueva columna con la categoría de precio correspondiente\n",
    "data['price_category'] = pd.cut(data['SalePrice'], [0, lim_inf, lim_sup, data['SalePrice'].max()], labels=['baratas', 'medias', 'caras'])\n",
    "\n",
    "# Mostrar las primeras filas del dataframe con la nueva columna\n",
    "print(data.head())\n",
    "\n",
    "checkingcategory = pd.get_dummies(data['price_category'])\n",
    "checkingcategory.head()\n",
    "\n",
    "data = pd.concat([data, checkingcategory], axis=1)\n",
    "baratas = data.pop('baratas')\n",
    "medias = data.pop('medias')\n",
    "caras = data.pop('caras')\n",
    "data.pop('price_category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, pd.concat([baratas, medias, caras], axis=1), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo 1 - Loss: 15.6734, Accuracy: 0.6895\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# Modelo 1\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(32, input_dim=X_train.shape[1], activation='relu'))\n",
    "model1.add(Dense(3, activation='softmax'))\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo 1\n",
    "model1.fit(X_train, y_train, epochs=50, batch_size=64, verbose=0)\n",
    "\n",
    "# Evaluar el modelo 1 en el test set\n",
    "loss, accuracy = model1.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Modelo 1 - Loss: {:.4f}, Accuracy: {:.4f}\".format(loss, accuracy))\n",
    "\n",
    "#neurona con mayor prob\n",
    "\n",
    "predictionmodel1 = model1.predict(X_test)\n",
    "classpredictmodel1 = np.argmax(model1.predict(X_test), axis=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo 2 - Loss: 3.6692, Accuracy: 0.8265\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "7/7 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "#modelo 2\n",
    "\n",
    "# Crear modelo\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(64, input_shape=(data.shape[1],)))\n",
    "model2.add(LeakyReLU(alpha=0.1))\n",
    "model2.add(Dense(32))\n",
    "model2.add(LeakyReLU(alpha=0.1))\n",
    "model2.add(Dense(16))\n",
    "model2.add(LeakyReLU(alpha=0.1))\n",
    "model2.add(Dense(8))\n",
    "model2.add(LeakyReLU(alpha=0.1))\n",
    "model2.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compilar modelo\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo 2\n",
    "model2.fit(X_train, y_train, epochs=50, batch_size=64, verbose=0)\n",
    "\n",
    "# Evaluar el modelo 2 en el test set\n",
    "loss, accuracy = model2.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Modelo 2 - Loss: {:.4f}, Accuracy: {:.4f}\".format(loss, accuracy))\n",
    "\n",
    "#neurona con mayor prob\n",
    "\n",
    "predictionmodel2 = model2.predict(X_test)\n",
    "classpredictmodel2 = np.argmax(model2.predict(X_test), axis=-1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Haga las matrices de confusión respectivas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[76  0  0]\n",
      " [44  0 24]\n",
      " [ 0  0 75]]\n"
     ]
    }
   ],
   "source": [
    "matrizdeconfusion1 = confusion_matrix(y_true=np.argmax(y_test.values, axis=-1), y_pred=classpredictmodel1)\n",
    "print(matrizdeconfusion1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[76  0  0]\n",
      " [16 52  0]\n",
      " [ 0 22 53]]\n"
     ]
    }
   ],
   "source": [
    "matrizdeconfusion2 = confusion_matrix(y_true=np.argmax(y_test.values, axis=-1), y_pred=classpredictmodel2)\n",
    "print(matrizdeconfusion2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Compare los resultados obtenidos con los diferentes modelos de clasificación usando redes \n",
    "neuronales en cuanto a efectividad, tiempo de procesamiento y equivocaciones (donde el \n",
    "algoritmo se equivocó más, donde se equivocó menos y la importancia que tienen los \n",
    "errores)\n",
    "\n",
    "R//:El modelo de clasificación multiclase con dos capas densas es el más eficaz teniendo los mejores resultados y mejorando después de cada run, por lo tanto es el mas recomendable. Teniendo un rendimiento de 88% a comparacion del otro modelo que esta en 68% de rendimiento."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Analice si no hay sobreajuste en los modelos. Use para esto la curva de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot clone object '<keras.engine.sequential.Sequential object at 0x000001A33C2D1D90>' (type <class 'keras.engine.sequential.Sequential'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' method.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\alegu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:825\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    824\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     tasks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ready_batches\u001b[39m.\u001b[39;49mget(block\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    826\u001b[0m \u001b[39mexcept\u001b[39;00m queue\u001b[39m.\u001b[39mEmpty:\n\u001b[0;32m    827\u001b[0m     \u001b[39m# slice the iterator n_jobs * batchsize items at a time. If the\u001b[39;00m\n\u001b[0;32m    828\u001b[0m     \u001b[39m# slice returns less than that, then the current batchsize puts\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    831\u001b[0m     \u001b[39m# accordingly to distribute evenly the last items between all\u001b[39;00m\n\u001b[0;32m    832\u001b[0m     \u001b[39m# workers.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alegu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\queue.py:168\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qsize():\n\u001b[1;32m--> 168\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[0;32m    169\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[263], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n\u001b[0;32m     22\u001b[0m \u001b[39m# Generar la curva de aprendizaje para el modelo 1\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m plot_learning_curve(model1, X_train, y_train)\n",
      "Cell \u001b[1;32mIn[263], line 3\u001b[0m, in \u001b[0;36mplot_learning_curve\u001b[1;34m(model, X, y)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot_learning_curve\u001b[39m(model, X, y):\n\u001b[1;32m----> 3\u001b[0m     train_sizes, train_scores, test_scores \u001b[39m=\u001b[39m learning_curve(\n\u001b[0;32m      4\u001b[0m         model, X, y, cv\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, n_jobs\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, train_sizes\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mlinspace(\u001b[39m0.1\u001b[39;49m, \u001b[39m1.0\u001b[39;49m, \u001b[39m10\u001b[39;49m), scoring\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39maccuracy\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[0;32m      5\u001b[0m     )\n\u001b[0;32m      6\u001b[0m     train_scores_mean \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(train_scores, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      7\u001b[0m     train_scores_std \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstd(train_scores, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\alegu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:1597\u001b[0m, in \u001b[0;36mlearning_curve\u001b[1;34m(estimator, X, y, groups, train_sizes, cv, scoring, exploit_incremental_learning, n_jobs, pre_dispatch, verbose, shuffle, random_state, error_score, return_times, fit_params)\u001b[0m\n\u001b[0;32m   1594\u001b[0m     \u001b[39mfor\u001b[39;00m n_train_samples \u001b[39min\u001b[39;00m train_sizes_abs:\n\u001b[0;32m   1595\u001b[0m         train_test_proportions\u001b[39m.\u001b[39mappend((train[:n_train_samples], test))\n\u001b[1;32m-> 1597\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m   1598\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m   1599\u001b[0m         clone(estimator),\n\u001b[0;32m   1600\u001b[0m         X,\n\u001b[0;32m   1601\u001b[0m         y,\n\u001b[0;32m   1602\u001b[0m         scorer,\n\u001b[0;32m   1603\u001b[0m         train,\n\u001b[0;32m   1604\u001b[0m         test,\n\u001b[0;32m   1605\u001b[0m         verbose,\n\u001b[0;32m   1606\u001b[0m         parameters\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   1607\u001b[0m         fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[0;32m   1608\u001b[0m         return_train_score\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1609\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m   1610\u001b[0m         return_times\u001b[39m=\u001b[39;49mreturn_times,\n\u001b[0;32m   1611\u001b[0m     )\n\u001b[0;32m   1612\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m train_test_proportions\n\u001b[0;32m   1613\u001b[0m )\n\u001b[0;32m   1614\u001b[0m results \u001b[39m=\u001b[39m _aggregate_score_dicts(results)\n\u001b[0;32m   1615\u001b[0m train_scores \u001b[39m=\u001b[39m results[\u001b[39m\"\u001b[39m\u001b[39mtrain_scores\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, n_unique_ticks)\u001b[39m.\u001b[39mT\n",
      "File \u001b[1;32mc:\\Users\\alegu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\alegu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1048\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1040\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 1048\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1049\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32mc:\\Users\\alegu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:836\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    833\u001b[0m n_jobs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_effective_n_jobs\n\u001b[0;32m    834\u001b[0m big_batch_size \u001b[39m=\u001b[39m batch_size \u001b[39m*\u001b[39m n_jobs\n\u001b[1;32m--> 836\u001b[0m islice \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(itertools\u001b[39m.\u001b[39;49mislice(iterator, big_batch_size))\n\u001b[0;32m    837\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(islice) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alegu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:59\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39m# Capture the thread-local scikit-learn configuration at the time\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[39m# Parallel.__call__ is issued since the tasks can be dispatched\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# in a different thread depending on the backend and on the value of\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# pre_dispatch and n_jobs.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m---> 59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[0;32m     63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\alegu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:1599\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1594\u001b[0m     \u001b[39mfor\u001b[39;00m n_train_samples \u001b[39min\u001b[39;00m train_sizes_abs:\n\u001b[0;32m   1595\u001b[0m         train_test_proportions\u001b[39m.\u001b[39mappend((train[:n_train_samples], test))\n\u001b[0;32m   1597\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m   1598\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m-> 1599\u001b[0m         clone(estimator),\n\u001b[0;32m   1600\u001b[0m         X,\n\u001b[0;32m   1601\u001b[0m         y,\n\u001b[0;32m   1602\u001b[0m         scorer,\n\u001b[0;32m   1603\u001b[0m         train,\n\u001b[0;32m   1604\u001b[0m         test,\n\u001b[0;32m   1605\u001b[0m         verbose,\n\u001b[0;32m   1606\u001b[0m         parameters\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1607\u001b[0m         fit_params\u001b[39m=\u001b[39mfit_params,\n\u001b[0;32m   1608\u001b[0m         return_train_score\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   1609\u001b[0m         error_score\u001b[39m=\u001b[39merror_score,\n\u001b[0;32m   1610\u001b[0m         return_times\u001b[39m=\u001b[39mreturn_times,\n\u001b[0;32m   1611\u001b[0m     )\n\u001b[0;32m   1612\u001b[0m     \u001b[39mfor\u001b[39;00m train, test \u001b[39min\u001b[39;00m train_test_proportions\n\u001b[0;32m   1613\u001b[0m )\n\u001b[0;32m   1614\u001b[0m results \u001b[39m=\u001b[39m _aggregate_score_dicts(results)\n\u001b[0;32m   1615\u001b[0m train_scores \u001b[39m=\u001b[39m results[\u001b[39m\"\u001b[39m\u001b[39mtrain_scores\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, n_unique_ticks)\u001b[39m.\u001b[39mT\n",
      "File \u001b[1;32mc:\\Users\\alegu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:79\u001b[0m, in \u001b[0;36mclone\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m     74\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mCannot clone object. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     75\u001b[0m                 \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mYou should provide an instance of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     76\u001b[0m                 \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mscikit-learn estimator instead of a class.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     77\u001b[0m             )\n\u001b[0;32m     78\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 79\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m     80\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mCannot clone object \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m (type \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m): \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     81\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mit does not seem to be a scikit-learn \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     82\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mestimator as it does not implement a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     83\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mget_params\u001b[39m\u001b[39m'\u001b[39m\u001b[39m method.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mrepr\u001b[39m(estimator), \u001b[39mtype\u001b[39m(estimator))\n\u001b[0;32m     84\u001b[0m             )\n\u001b[0;32m     86\u001b[0m klass \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\n\u001b[0;32m     87\u001b[0m new_object_params \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39mget_params(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot clone object '<keras.engine.sequential.Sequential object at 0x000001A33C2D1D90>' (type <class 'keras.engine.sequential.Sequential'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' method."
     ]
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Para el modelo elegido de clasificación tunee los parámetros y discuta si puede mejorar \n",
    "todavía el modelo sin llegar a sobre ajustarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Seleccione ahora el SalesPrice como variable respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Genere dos modelos de regresión con redes neuronales con diferentes topologías y \n",
    "funciones de activación para predecir el precio de las casas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Compare los dos modelos de regresión y determine cuál funcionó mejor para predecir el \n",
    "precio de las casas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Analice si no hay sobreajuste en los modelos. Use para esto la curva de aprendizaje.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Para el modelo elegido de regresión tunee los parámetros y discuta si puede mejorar \n",
    "todavía el modelo sin llegar a sobre ajustarlo. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Compare la eficiencia del mejor modelo de RNA con los resultados obtenidos con los \n",
    "algoritmos de las hojas de trabajo anteriores. ¿Cuál es mejor para predecir? ¿Cuál se \n",
    "demoró más en procesar?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Compare los resultados del mejor modelo de esta hoja para clasificar con los resultados de \n",
    "los algoritmos usados para clasificar de las hojas de trabajo anteriores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. Compare los resultados del mejor modelo para predecir el precio de venta con los \n",
    "resultados de los algoritmos usados para el mismo propósito de las hojas de trabajo \n",
    "anteriores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. Ahora que ha usado todos los modelos que hemos visto y aplicados al conjunto de datos \n",
    "llegue a conclusiones sobre cual es o cuales son los mejores modelos para clasificar dadas \n",
    "las características del conjunto de datos. ¿Cuál o cuáles son los mejores para predecir el \n",
    "precio de las casas? Una tabla de resumen con las métricas de los modelos le puede resultar \n",
    "muy útil para esto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. Genere un informe de los resultados y las explicaciones."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
